{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "원티드_프리온보딩_코스_안정이.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pxpper/wanted_pre_onboarding/blob/main/%EC%9B%90%ED%8B%B0%EB%93%9C_%ED%94%84%EB%A6%AC%EC%98%A8%EB%B3%B4%EB%94%A9_%EC%BD%94%EC%8A%A4_%EC%95%88%EC%A0%95%EC%9D%B4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "vX6CuXeZ9xWA"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from math import log\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Tokenizer:\n",
        "  def __init__(self):\n",
        "    self.word_dict = {'oov': 0}\n",
        "    self.fit_checker = False\n",
        "  \n",
        "  def preprocessing(self, sequences):\n",
        "    result = []\n",
        "    for seq in sequences:\n",
        "        seq = seq.strip()\n",
        "        seq = seq.lower()\n",
        "        seq = re.sub('[^\\w\\s]','',seq)\n",
        "        temp = seq.split()\n",
        "        result.append(temp)\n",
        "    return result\n",
        "  \n",
        "  def fit(self, sequences):\n",
        "    self.fit_checker = False\n",
        "    seq_list = self.preprocessing(sequences)\n",
        "    dic_val = 1\n",
        "    for seq in seq_list:\n",
        "        for token in seq:\n",
        "            if token not in self.word_dict.keys():\n",
        "                self.word_dict[token] = dic_val\n",
        "                dic_val += 1\n",
        "    self.fit_checker = True\n",
        "    \n",
        "  def transform(self, sequences):\n",
        "    result = []\n",
        "    seq_list = self.preprocessing(sequences)\n",
        "    if self.fit_checker:\n",
        "      for seq in seq_list:\n",
        "          temp = []\n",
        "          for token in seq:\n",
        "              if token in self.word_dict.keys():\n",
        "                  temp.append(self.word_dict[token])\n",
        "              else:\n",
        "                  temp.append(0)\n",
        "          result.append(temp)\n",
        "      return result\n",
        "    else:\n",
        "      raise Exception(\"Tokenizer instance is not fitted yet.\")\n",
        "      \n",
        "  def fit_transform(self, sequences):\n",
        "    self.fit(sequences)\n",
        "    result = self.transform(sequences)\n",
        "    return result"
      ],
      "metadata": {
        "id": "hfrAmg7l936h"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TfidfVectorizer:\n",
        "  def __init__(self, tokenizer):\n",
        "    self.tokenizer = Tokenizer()\n",
        "    self.fit_checker = False\n",
        "  \n",
        "  def fit(self, sequences):\n",
        "    tokenized = self.tokenizer.fit_transform(sequences)\n",
        "    index_list = list(set(sum(tokenized, [])))\n",
        "    \n",
        "    counting_list = np.zeros(len(index_list))\n",
        "\n",
        "    for token in tokenized:\n",
        "        for i, voca in enumerate(index_list):\n",
        "            if voca in token:\n",
        "              counting_list[i] += 1\n",
        "    \n",
        "    temp=[]\n",
        "    for x in counting_list:\n",
        "      temp.append(log(len(tokenized)/(1+x)))\n",
        "    self.idf = temp\n",
        "    \n",
        "    self.fit_checker = True\n",
        "    \n",
        "\n",
        "  def transform(self, sequences):\n",
        "    if self.fit_checker:\n",
        "      tokenized = self.tokenizer.transform(sequences)\n",
        "      index_list = list(set(sum(tokenized, [])))\n",
        "      \n",
        "      result = []\n",
        "      for i, token in enumerate(tokenized):\n",
        "        temp = []\n",
        "        for j, index in enumerate(index_list):\n",
        "          tf = token.count(index)\n",
        "          temp.append(tf * self.idf[j])\n",
        "        result.append(temp)\n",
        "      return result\n",
        "    else:\n",
        "      raise Exception(\"TfidfVectorizer instance is not fitted yet.\")\n",
        "\n",
        "  \n",
        "  def fit_transform(self, sequences):\n",
        "    self.fit(sequences)\n",
        "    return self.transform(sequences)"
      ],
      "metadata": {
        "id": "qVaUmh-597_-"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "P1-1. tokenizered result"
      ],
      "metadata": {
        "id": "GMh2JYqE-KFT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer=Tokenizer()\n",
        "seq_list=tokenizer.preprocessing(sequences=['I go to school.', 'I LIKE pizza!'])\n",
        "print(seq_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XqvcHkgd-Kz_",
        "outputId": "af8891e8-a153-43b0-919d-6d916b7f63c6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['i', 'go', 'to', 'school'], ['i', 'like', 'pizza']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "P1-2. dict"
      ],
      "metadata": {
        "id": "rzKONHBj-TuN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.fit(sequences=['I go to school.', 'I LIKE pizza!'])\n",
        "print(tokenizer.word_dict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y9ZF5ALm-Wb7",
        "outputId": "2667876b-232e-4321-aad8-cb82e7531343"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'oov': 0, 'i': 1, 'go': 2, 'to': 3, 'school': 4, 'like': 5, 'pizza': 6}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " P1-3. indexing"
      ],
      "metadata": {
        "id": "ErXsrR5E-Ygu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "index_tensor=tokenizer.transform(sequences=['I go to school.', 'I LIKE pizza!'])\n",
        "print(index_tensor)\n",
        "\n",
        "print(\"조건 1: 어휘 사전(self.word_dict)에 없는 단어는 'oov'의 index로 변환합니다.\")\n",
        "index_tensor=tokenizer.transform(sequences=['I go to school.', 'I LIKE pizza and chips!'])\n",
        "print(index_tensor)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "as1Ky_xh-cw9",
        "outputId": "fe426e36-266f-4b56-f14c-45c8dbfed6fd"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1, 2, 3, 4], [1, 5, 6]]\n",
            "조건 1: 어휘 사전(self.word_dict)에 없는 단어는 'oov'의 index로 변환합니다.\n",
            "[[1, 2, 3, 4], [1, 5, 6, 0, 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "P2-1. IDF_array"
      ],
      "metadata": {
        "id": "L-gQVsp7-jsD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer=TfidfVectorizer(tokenizer)\n",
        "vectorizer.fit(sequences=['I go to school.', 'I LIKE pizza!'])\n",
        "print(vectorizer.idf)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xwinlumm-jRt",
        "outputId": "1b87b9fa-a364-4729-b7b7-022a30c829c7"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-0.40546510810816444, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "P2-2. TF-IDF_array"
      ],
      "metadata": {
        "id": "oqBcFLn9-pGT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_array=vectorizer.transform(sequences=['I go to school.', 'I LIKE pizza!'])\n",
        "print(tfidf_array)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H6kr3Z9t-pQX",
        "outputId": "9a12db51-8ced-4ba2-9b54-0b3a2cd40334"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-0.40546510810816444, 0.0, 0.0, 0.0, 0.0, 0.0], [-0.40546510810816444, 0.0, 0.0, 0.0, 0.0, 0.0]]\n"
          ]
        }
      ]
    }
  ]
}